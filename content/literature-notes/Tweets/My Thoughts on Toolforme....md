---
author: [[@mathemagic1an on Twitter]]
title: "My Thoughts on Toolforme..."
tags: 
- tweets
- literature-note
---
# My Thoughts on Toolforme...

![rw-book-cover](https://pbs.twimg.com/profile_images/1607042017095258114/sSUQlvdW.jpg)

## Metadata
- Author: [[@mathemagic1an on Twitter]]
- Full Title: My Thoughts on Toolforme...
- Category: #tweets
- URL: https://twitter.com/mathemagic1an/status/1624870248221663232

## Highlights
- My thoughts on Toolformer
  IMO the most important paper in the past few weeks.
  https://t.co/4IDciigbkc
  Teach an LLM to use tools, like a calculator or search engine, in a *self-supervised manner*
  Interesting hack to resolve many blind spots of current LLMs
  Here's how ðŸ‘‡ 
  ![](https://pbs.twimg.com/media/FoypEmUakAA9g8z.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870248221663232))
- The idea with Toolformer:
  Get an LLM to make API requests by generating specific token subsequences, like CALC(1*2)
  Then give the LLM the API results in it's input
  Example flow illustrated below, for a calculator API
  2/ 
  ![](https://pbs.twimg.com/media/FoyrIPCaAAAaQtr.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870249349910528))
- Not only does this work encouragingly well for certain tasks
  It's also trained in a self-supervised fashion!
  This means: from a small seed set of human inputs (essentially demonstrating usage of APIs), the training set for this behavior is generated by the LLM itself.
  3/ 
  ![](https://pbs.twimg.com/media/Foyr48xaEAAC6vu.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870250453041153))
- Their training process is as follows:
  - take a regular dataset
  - generate/insert "candidate" API calls into samples from this dataset
  - have the LLM view these annotated samples and see if the API calls were actually helpful
  - fine-tune the LLM on these "helpful" samples
  4/ 
  ![](https://pbs.twimg.com/media/Foysg5FaIAA9Uar.png) 
  ![](https://pbs.twimg.com/media/Foyvu05acAAgzQ3.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870251539337216))
- For certain tasks, Toolformer shows a ton of lift
  It is comparable to (or way better than) GPT-3 on several tasks considered "blind spots" for LLMs, most notably in questions involving math/arithmetic
  And GPT-3 has 25x more parameters!
  5/ 
  ![](https://pbs.twimg.com/media/Foys6LuaYAAKnBO.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870252655034368))
- Toolformer has some reasonable limitations:
  - It can't directly "chain" API calls
  - It's limited to non-interactive, textual tools
  - It takes ~1,000 documents to generate a single usable API call sample for fine-tuning
  6/ 
  ![](https://pbs.twimg.com/media/Foyw5XPaIAAI43D.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870254819115012))
- However, the authors haven't even tried doing this training process iteratively yet!
  (You could use a pre-trained toolformer to bootstrap an even more comprehensive dataset, with more complex usages of APIs etc., then repeat.)
  Major potential upside. Someone should do this.
  7/ 
  ![](https://pbs.twimg.com/media/FoyxaRTakAAhCJq.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870256564133888))
- So what does this mean?
  We've found a promising way to tightly integrate arbitrary APIs with our best-performing models.
  This approach is both scalable (self-supervised) and doesn't require significant modifications to existing LLM architectures to work
  8/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870257818222592))
- That is, there's minimal effort required for model API providers to "enhance" their existing offerings with a technique like this ^
  OpenAI could start offering cybernetically-enhanced LLMs on tap tomorrow and you'd see it suddenly get 5x better at arithmetic.
  9/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870259076501505))
- Or, if you want to integrate your own, custom tool, I'm sure @LangChainAI or equivalent will provide an implementation for interpreting Toolformer models' outputs + executing API calls.
  Similar to reAct agents: https://t.co/J9XGJkLD4b
  ðŸ™Œ
  8/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870259982487552))
- Generally, there's a recent trend of big wins from self-supervision
  "Chinchilla laws" tell us you need a TON of data to train better models, and we've exhausted the whole web
  Using AI to generate new training data, however, is surprisingly effective
  https://t.co/mVS5dpNhE2
  9/ 
  ![](https://pbs.twimg.com/media/Foyuk-zacAAcCXv.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870260942995456))
- The takeaway:
  - LLMs grow more powerful by the day
  - Many quick wins by simple extensions to existing models
  - Data limitations may not be as limiting as we thought. ([View Tweet](https://twitter.com/mathemagic1an/status/1624870262129950723))
---
author: [[@mathemagic1an on Twitter]]
title: "My Thoughts on Toolforme..."
tags: 
- tweets
- literature-note
---
# My Thoughts on Toolforme...

![rw-book-cover](https://pbs.twimg.com/profile_images/1607042017095258114/sSUQlvdW.jpg)

## Metadata
- Author: [[@mathemagic1an on Twitter]]
- Full Title: My Thoughts on Toolforme...
- Category: #tweets
- URL: https://twitter.com/mathemagic1an/status/1624870248221663232

## Highlights
- My thoughts on Toolformer
  IMO the most important paper in the past few weeks.
  https://t.co/4IDciigbkc
  Teach an LLM to use tools, like a calculator or search engine, in a *self-supervised manner*
  Interesting hack to resolve many blind spots of current LLMs
  Here's how ðŸ‘‡ 
  ![](https://pbs.twimg.com/media/FoypEmUakAA9g8z.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870248221663232))
- The idea with Toolformer:
  Get an LLM to make API requests by generating specific token subsequences, like CALC(1*2)
  Then give the LLM the API results in it's input
  Example flow illustrated below, for a calculator API
  2/ 
  ![](https://pbs.twimg.com/media/FoyrIPCaAAAaQtr.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870249349910528))
- Not only does this work encouragingly well for certain tasks
  It's also trained in a self-supervised fashion!
  This means: from a small seed set of human inputs (essentially demonstrating usage of APIs), the training set for this behavior is generated by the LLM itself.
  3/ 
  ![](https://pbs.twimg.com/media/Foyr48xaEAAC6vu.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870250453041153))
- Their training process is as follows:
  - take a regular dataset
  - generate/insert "candidate" API calls into samples from this dataset
  - have the LLM view these annotated samples and see if the API calls were actually helpful
  - fine-tune the LLM on these "helpful" samples
  4/ 
  ![](https://pbs.twimg.com/media/Foysg5FaIAA9Uar.png) 
  ![](https://pbs.twimg.com/media/Foyvu05acAAgzQ3.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870251539337216))
- For certain tasks, Toolformer shows a ton of lift
  It is comparable to (or way better than) GPT-3 on several tasks considered "blind spots" for LLMs, most notably in questions involving math/arithmetic
  And GPT-3 has 25x more parameters!
  5/ 
  ![](https://pbs.twimg.com/media/Foys6LuaYAAKnBO.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870252655034368))
- Toolformer has some reasonable limitations:
  - It can't directly "chain" API calls
  - It's limited to non-interactive, textual tools
  - It takes ~1,000 documents to generate a single usable API call sample for fine-tuning
  6/ 
  ![](https://pbs.twimg.com/media/Foyw5XPaIAAI43D.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870254819115012))
- So what does this mean?
  We've found a promising way to tightly integrate arbitrary APIs with our best-performing models.
  This approach is both scalable (self-supervised) and doesn't require significant modifications to existing LLM architectures to work
  8/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870257818222592))
- That is, there's minimal effort required for model API providers to "enhance" their existing offerings with a technique like this ^
  OpenAI could start offering cybernetically-enhanced LLMs on tap tomorrow and you'd see it suddenly get 5x better at arithmetic.
  9/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870259076501505))
- Or, if you want to integrate your own, custom tool, I'm sure @LangChainAI or equivalent will provide an implementation for interpreting Toolformer models' outputs + executing API calls.
  Similar to reAct agents: https://t.co/J9XGJkLD4b
  ðŸ™Œ
  8/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870259982487552))
- Generally, there's a recent trend of big wins from self-supervision
  "Chinchilla laws" tell us you need a TON of data to train better models, and we've exhausted the whole web
  Using AI to generate new training data, however, is surprisingly effective
  https://t.co/mVS5dpNhE2
  9/ 
  ![](https://pbs.twimg.com/media/Foyuk-zacAAcCXv.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870260942995456))
- The takeaway:
  - LLMs grow more powerful by the day
  - Many quick wins by simple extensions to existing models
  - Data limitations may not be as limiting as we thought. ([View Tweet](https://twitter.com/mathemagic1an/status/1624870262129950723))
---
author: [[@mathemagic1an on Twitter]]
title: "My Thoughts on Toolforme..."
tags: 
- tweets
- literature-note
---
# My Thoughts on Toolforme...

![rw-book-cover](https://pbs.twimg.com/profile_images/1607042017095258114/sSUQlvdW.jpg)

## Metadata
- Author: [[@mathemagic1an on Twitter]]
- Full Title: My Thoughts on Toolforme...
- Category: #tweets
- URL: https://twitter.com/mathemagic1an/status/1624870248221663232

## Highlights
- My thoughts on Toolformer
  IMO the most important paper in the past few weeks.
  https://t.co/4IDciigbkc
  Teach an LLM to use tools, like a calculator or search engine, in a *self-supervised manner*
  Interesting hack to resolve many blind spots of current LLMs
  Here's how ðŸ‘‡ 
  ![](https://pbs.twimg.com/media/FoypEmUakAA9g8z.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870248221663232))
- The idea with Toolformer:
  Get an LLM to make API requests by generating specific token subsequences, like CALC(1*2)
  Then give the LLM the API results in it's input
  Example flow illustrated below, for a calculator API
  2/ 
  ![](https://pbs.twimg.com/media/FoyrIPCaAAAaQtr.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870249349910528))
- Not only does this work encouragingly well for certain tasks
  It's also trained in a self-supervised fashion!
  This means: from a small seed set of human inputs (essentially demonstrating usage of APIs), the training set for this behavior is generated by the LLM itself.
  3/ 
  ![](https://pbs.twimg.com/media/Foyr48xaEAAC6vu.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870250453041153))
- Their training process is as follows:
  - take a regular dataset
  - generate/insert "candidate" API calls into samples from this dataset
  - have the LLM view these annotated samples and see if the API calls were actually helpful
  - fine-tune the LLM on these "helpful" samples
  4/ 
  ![](https://pbs.twimg.com/media/Foysg5FaIAA9Uar.png) 
  ![](https://pbs.twimg.com/media/Foyvu05acAAgzQ3.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870251539337216))
- For certain tasks, Toolformer shows a ton of lift
  It is comparable to (or way better than) GPT-3 on several tasks considered "blind spots" for LLMs, most notably in questions involving math/arithmetic
  And GPT-3 has 25x more parameters!
  5/ 
  ![](https://pbs.twimg.com/media/Foys6LuaYAAKnBO.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870252655034368))
- Toolformer has some reasonable limitations:
  - It can't directly "chain" API calls
  - It's limited to non-interactive, textual tools
  - It takes ~1,000 documents to generate a single usable API call sample for fine-tuning
  6/ 
  ![](https://pbs.twimg.com/media/Foyw5XPaIAAI43D.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870254819115012))
- So what does this mean?
  We've found a promising way to tightly integrate arbitrary APIs with our best-performing models.
  This approach is both scalable (self-supervised) and doesn't require significant modifications to existing LLM architectures to work
  8/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870257818222592))
- That is, there's minimal effort required for model API providers to "enhance" their existing offerings with a technique like this ^
  OpenAI could start offering cybernetically-enhanced LLMs on tap tomorrow and you'd see it suddenly get 5x better at arithmetic.
  9/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870259076501505))
- Or, if you want to integrate your own, custom tool, I'm sure @LangChainAI or equivalent will provide an implementation for interpreting Toolformer models' outputs + executing API calls.
  Similar to reAct agents: https://t.co/J9XGJkLD4b
  ðŸ™Œ
  8/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870259982487552))
- Generally, there's a recent trend of big wins from self-supervision
  "Chinchilla laws" tell us you need a TON of data to train better models, and we've exhausted the whole web
  Using AI to generate new training data, however, is surprisingly effective
  https://t.co/mVS5dpNhE2
  9/ 
  ![](https://pbs.twimg.com/media/Foyuk-zacAAcCXv.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870260942995456))
- The takeaway:
  - LLMs grow more powerful by the day
  - Many quick wins by simple extensions to existing models
  - Data limitations may not be as limiting as we thought. ([View Tweet](https://twitter.com/mathemagic1an/status/1624870262129950723))
---
author: [[@mathemagic1an on Twitter]]
title: "My Thoughts on Toolforme..."
tags: 
- tweets
- literature-note
---
# My Thoughts on Toolforme...

![rw-book-cover](https://pbs.twimg.com/profile_images/1607042017095258114/sSUQlvdW.jpg)

## Metadata
- Author: [[@mathemagic1an on Twitter]]
- Full Title: My Thoughts on Toolforme...
- Category: #tweets
- URL: https://twitter.com/mathemagic1an/status/1624870248221663232

## Highlights
- My thoughts on Toolformer
  IMO the most important paper in the past few weeks.
  https://t.co/4IDciigbkc
  Teach an LLM to use tools, like a calculator or search engine, in a *self-supervised manner*
  Interesting hack to resolve many blind spots of current LLMs
  Here's how ðŸ‘‡ 
  ![](https://pbs.twimg.com/media/FoypEmUakAA9g8z.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870248221663232))
- The idea with Toolformer:
  Get an LLM to make API requests by generating specific token subsequences, like CALC(1*2)
  Then give the LLM the API results in it's input
  Example flow illustrated below, for a calculator API
  2/ 
  ![](https://pbs.twimg.com/media/FoyrIPCaAAAaQtr.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870249349910528))
- Not only does this work encouragingly well for certain tasks
  It's also trained in a self-supervised fashion!
  This means: from a small seed set of human inputs (essentially demonstrating usage of APIs), the training set for this behavior is generated by the LLM itself.
  3/ 
  ![](https://pbs.twimg.com/media/Foyr48xaEAAC6vu.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870250453041153))
- Their training process is as follows:
  - take a regular dataset
  - generate/insert "candidate" API calls into samples from this dataset
  - have the LLM view these annotated samples and see if the API calls were actually helpful
  - fine-tune the LLM on these "helpful" samples
  4/ 
  ![](https://pbs.twimg.com/media/Foysg5FaIAA9Uar.png) 
  ![](https://pbs.twimg.com/media/Foyvu05acAAgzQ3.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870251539337216))
- For certain tasks, Toolformer shows a ton of lift
  It is comparable to (or way better than) GPT-3 on several tasks considered "blind spots" for LLMs, most notably in questions involving math/arithmetic
  And GPT-3 has 25x more parameters!
  5/ 
  ![](https://pbs.twimg.com/media/Foys6LuaYAAKnBO.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870252655034368))
- Toolformer has some reasonable limitations:
  - It can't directly "chain" API calls
  - It's limited to non-interactive, textual tools
  - It takes ~1,000 documents to generate a single usable API call sample for fine-tuning
  6/ 
  ![](https://pbs.twimg.com/media/Foyw5XPaIAAI43D.jpg) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870254819115012))
- So what does this mean?
  We've found a promising way to tightly integrate arbitrary APIs with our best-performing models.
  This approach is both scalable (self-supervised) and doesn't require significant modifications to existing LLM architectures to work
  8/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870257818222592))
- That is, there's minimal effort required for model API providers to "enhance" their existing offerings with a technique like this ^
  OpenAI could start offering cybernetically-enhanced LLMs on tap tomorrow and you'd see it suddenly get 5x better at arithmetic.
  9/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870259076501505))
- Or, if you want to integrate your own, custom tool, I'm sure @LangChainAI or equivalent will provide an implementation for interpreting Toolformer models' outputs + executing API calls.
  Similar to reAct agents: https://t.co/J9XGJkLD4b
  ðŸ™Œ
  8/ ([View Tweet](https://twitter.com/mathemagic1an/status/1624870259982487552))
- Generally, there's a recent trend of big wins from self-supervision
  "Chinchilla laws" tell us you need a TON of data to train better models, and we've exhausted the whole web
  Using AI to generate new training data, however, is surprisingly effective
  https://t.co/mVS5dpNhE2
  9/ 
  ![](https://pbs.twimg.com/media/Foyuk-zacAAcCXv.png) ([View Tweet](https://twitter.com/mathemagic1an/status/1624870260942995456))
- The takeaway:
  - LLMs grow more powerful by the day
  - Many quick wins by simple extensions to existing models
  - Data limitations may not be as limiting as we thought. ([View Tweet](https://twitter.com/mathemagic1an/status/1624870262129950723))
