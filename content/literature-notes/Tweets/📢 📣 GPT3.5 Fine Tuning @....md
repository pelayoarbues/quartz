---
author: [[@NirantK on Twitter]]
title: "ğŸ“¢ ğŸ“£ GPT3.5 Fine Tuning @..."
date: 2023-09-13
tags: 
- tweets
- literature-note
---
![rw-book-cover](https://pbs.twimg.com/profile_images/1606637371167952896/9AZ5RnU2.jpg)

## Metadata
- Author: [[@NirantK on Twitter]]
- Full Title: ğŸ“¢ ğŸ“£ GPT3.5 Fine Tuning @...
- URL: https://twitter.com/NirantK/status/1701526502880547233

## Highlights
- ğŸ“¢ ğŸ“£ GPT3.5 Fine Tuning @OpenAI Cookbook for Retrieval Augmented Generation (RAG)
  - Guide to reduce Total Error by ~5 times!
  - Do few-shot training & inference with @qdrant_engine
  - Strike balance between accuracy & hallucinations
  ğŸ™ @colintjarvis for the swift reviews! 
  ![](https://pbs.twimg.com/media/F50JVcsagAAmMfP.jpg) ([View Tweet](https://twitter.com/NirantK/status/1701526502880547233))
- The Finetuning results? Absolutely mind-blowing ğŸ¤¯!
  1. Hallucinations cut in HALF
  2. Without sacrificing CORRECT answers!
  2. When there's an answer in the context, we "Skip" a measly 6% to 8%
  And guess what? Few Shot with @qdrant_engine makes it even more controllable! 
  ![](https://pbs.twimg.com/media/F50JV1ZbwAAph0q.png) ([View Tweet](https://twitter.com/NirantK/status/1701526509838864665))
- ğŸ“Š The notebook provides an example of how to think about Performance Evaluation â€” all human annotation samples, including those where the answer is "impossible" given the context!
  The error categories as in the graph above? Explained here 
  ![](https://pbs.twimg.com/media/F50JWPtaoAAvnMA.jpg) ([View Tweet](https://twitter.com/NirantK/status/1701526517715812579))
- ğŸ™Œ Whether you're an experienced professional or new to AI and ML, this notebook is a valuable resource for model fine-tuning and evaluation.
  https://t.co/WpZgK1BzKU ([View Tweet](https://twitter.com/NirantK/status/1701526520941207727))
