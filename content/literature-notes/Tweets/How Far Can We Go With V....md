---
author: [[@YutongBAI1002 on Twitter]]
title: "How Far Can We Go With V..."
date: 2023-12-04
tags: 
- tweets
- literature-note
---
![rw-book-cover](https://pbs.twimg.com/profile_images/1621614237167534084/PBtgGKiM.jpg)

## Metadata
- Author: [[@YutongBAI1002 on Twitter]]
- Full Title: How Far Can We Go With V...
- URL: https://twitter.com/YutongBAI1002/status/1731512082590478516

## Highlights
- How far can we go with vision alone? 
  Excited to reveal our Large Vision Model! Trained with 420B tokens, effective scalability, and enabling new avenues in vision tasks! (1/N)
  Kudos to <a href="https://twitter.com/younggeng">@younggeng</a> <a href="https://twitter.com/Karttikeya_m">@Karttikeya_m</a> <a href="https://twitter.com/_amirbar">@_amirbar</a>, <a href="https://twitter.com/YuilleAlan">@YuilleAlan</a> Trevor Darrell <a href="https://twitter.com/JitendraMalikCV">@JitendraMalikCV</a> Alyosha Efros! ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512082590478516))
- (2/N) We want to exploit all the remarkable diversity in visual data. Raw unannotated images and videos; semantic segmentations, depth reconstructions, multiple views of 3D objects, among others. The total size of our training dataset is 1.64 B images across 50 datasets. ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512084339237092))
- (3/N) To do this, we define a common format, "visual sentences", in which we can represent raw images and videos as well as annotated data sources such as semantic segmentations and depth reconstructions without needing any meta-knowledge beyond the pixels. 
  ![](https://pbs.twimg.com/media/GAePDG_bwAAD0TR.jpg) ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512086503805053))
- (4/N) LVM shows remarkable scalability on both model size and data size. 
  ![](https://pbs.twimg.com/media/GAePZIFbcAANat4.jpg) ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512089825698166))
- (5/N) Shows flexible specification of tasks through prompting (in-context learning). Like Video frames prediction: 
  ![](https://pbs.twimg.com/media/GAeQCtaasAATOh5.jpg) ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512092539408484))
- (6/N) Analogy Prompting for tasks: 
  ![](https://pbs.twimg.com/media/GAeQIayaQAAiUDx.jpg) ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512095425052761))
- (7/N) More complex tasks: 
  ![](https://pbs.twimg.com/media/GAeQPFyacAAFOEf.jpg) ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512099279704548))
- (8/N) And Miscellaneous Prompting: 
  ![](https://pbs.twimg.com/media/GAeQY9VaIAA9jzq.jpg) ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512102232502642))
- (9/N) Tasks that are not always easily describable in language: 
  ![](https://pbs.twimg.com/media/GAeQci9acAA-eIa.jpg) ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512104845475892))
- (10/N) Non-verbal IQ test. (Raven's Progressive Matrics) 
  ![](https://pbs.twimg.com/media/GAeQ7Nba0AAVUgI.jpg) ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512107705983052))
- More information please check: 
  https://t.co/aZHIaQbUPY
  and https://t.co/dSJmdOU1vX ([View Tweet](https://twitter.com/YutongBAI1002/status/1731512110247473608))
