---
author: [[The Batch @ DeepLearning.AI]]
title: "Tumbling Training Costs, Desktop AI Supercomputer, Tighter AI Export Restrictions, Improved Contrastive Loss"
date: 2025-01-16
tags: 
- articles
- literature-note
---
![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article2.74d541386bbf.png)

## Metadata
- Author: [[The Batch @ DeepLearning.AI]]
- Full Title: Tumbling Training Costs, Desktop AI Supercomputer, Tighter AI Export Restrictions, Improved Contrastive Loss

## Highlights
- Writing software, especially prototypes, is becoming cheaper. This will lead to increased demand for people who can decide what to build. AI Product Management has a bright future! ([View Highlight](https://read.readwise.io/read/01jhqcdjpbn76as31dyg202ty0))
- Software is often written by teams that comprise Product Managers (PMs), who decide what to build (such as what features to implement for what users) and Software Developers, who write the code to build the product. Economics shows that when two goods are complements — such as cars (with internal-combustion engines) and gasoline — falling prices in one leads to higher demand for the other. For example, as cars became cheaper, more people bought them, which led to increased demand for gas. Something similar will happen in software. Given a clear specification for what to build, AI is making the building itself much faster and cheaper. This will significantly increase demand for people who can come up with clear specs for valuable things to build. ([View Highlight](https://read.readwise.io/read/01jhqce4da452wzfenhmxm93ja))
- This is why I’m excited about the future of Product Management, the discipline of developing and managing software products. I’m especially excited about the future of AI Product Management, the discipline of developing and managing AI software products. ([View Highlight](https://read.readwise.io/read/01jhqcea1gjky0phmns2f7k72e))
- Many companies have an Engineer:PM ratio of, say, 6:1. (The ratio varies widely by company and industry, and anywhere from 4:1 to 10:1 is typical.) As coding becomes more efficient, I think teams will need more product management work (as well as design work) as a fraction of the total workforce. Perhaps engineers will step in to do some of this work, but if it remains the purview of specialized Product Managers, then the demand for these roles will grow. ([View Highlight](https://read.readwise.io/read/01jhqcetp3ccjmkpx5s8mfc0y1))
- This change in the composition of software development teams is not yet moving forward at full speed. One major force slowing this shift, particularly in AI Product Management, is that Software Engineers, being technical, are understanding and embracing AI much faster than Product Managers. Even today, most companies have difficulty finding people who know how to develop products and also understand AI, and I expect this shortage to grow. ([View Highlight](https://read.readwise.io/read/01jhqcf1g4xvgjq8h8rgqx1r4f))
- Further, AI Product Management requires a different set of skills than traditional software Product Management. It requires:
  • **Technical proficiency in AI.** PMs need to understand what products might be technically feasible to build. They also need to understand the lifecycle of AI projects, such as data collection, building, then monitoring, and maintenance of AI models. 
  • **Iterative development.** Because AI development is much more iterative than traditional software and requires more course corrections along the way, PMs need to understand how to manage such a process. 
  • **Data proficiency.** AI products often learn from data, and they can be designed to generate richer forms of data than traditional software. 
  • **Skill in managing ambiguity.** Because AI’s performance is hard to predict in advance, PMs need to be comfortable with this and have tactics to manage it. 
  • **Ongoing learning.** AI technology is advancing rapidly. PMs, like everyone else who aims to make best use of the technology, need to keep up with the latest technology advances, product ideas, and how they fit into users’ lives. ([View Highlight](https://read.readwise.io/read/01jhqcfgdaf3hgb973evgg5wgs))
- Finally, AI Product Managers will need to know how to ensure that AI is implemented responsibly (for example, when we need to implement guardrails to prevent bad outcomes), and also be skilled at [gathering feedback fast](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV8l3qgyTW95jsWP6lZ3mdW4bVLL19bPfscW7FH55s8M-z5LMyTmGvQZKqbW8NMSDm3JP5rPW2GH57B1ZC2MlV_WthJ7Q63jsW4NDvzM6zp480W5C59MH1LjrG4W5gGsgL3y5RCCW2T43fR3LZpnsN1HYRdTrVY8DN1ssRcJlRBFhN1wHQLqFzx1zW1Xd91v2Fj2fqVJ8L3n2_TVNYW6SRqcw8Vp9_QV35pCh3W3v75N9bcMvRj-sytW2zfxm91bhVxmW7Ln7RY5nBfffW90MDLS4Y3nKwW99nHxg6nHsRbW8zJ_Hd1sNlx4W4Tz9fY4G2P4dW4xPXk02KJK3JW1vMQ_s7rqJ27N5NM2t6vTv5JW1DsrYQ53Lp2qW9m2_9F14C-ymW1DJpNJ5PKB-0cDmv004) to keep projects moving. Increasingly, I also expect strong product managers to be able to [build prototypes](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV823qgyTW8wLKSR6lZ3m4V90P6w7Wrc8jW2PKWPw1jLfFSW6tzQr_4TqBrCW4m63Dw4SS6H6W1TjpTX3FYvCzW8NVywg4Tn3QgW4vnMbJ6rlrLXW1tknNx6hH4ZxVSMPwG4K7GqyW1R3kqP7TNGHzW4LZ66l26hrYyW28B0zb66Y1cNW4vdxX58sdlFsW1xgh1c9dbTz_VShkgL2B-1y0N4WSBFDLKPtCW22gtyJ4Qjl6PW7gCV4N3TDQBjW5kQsyb1Z3VLWW4jVjqg1ZkbSvW9c3xfH7cKYvSW2mKnh796cy4hW8y_8Rz59l6GYW8tQKHX3C4bFgW6-zyjS7dwGwXW7PN_Db3CBBVLW3bndF35pVmCRW3LN2VP3qbVt1f2Qh_zC04) for themselves . ([View Highlight](https://read.readwise.io/read/01jhqcfmgp9b9c06hp8vzvd4c3))
- The demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work. ([View Highlight](https://read.readwise.io/read/01jhqcwmnbewf198n5jxgxr85t))


## New highlights added January 16, 2025 at 3:29 PM
- [DeepSeek-V3](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV783qgyTW6N1vHY6lZ3ktW9bLK9j5Ld40hF42S8ByN-8GW9cC7Mr2J7R47W965lsk5kPV1HVTx8175Rwz6JW4_73dY6FnzSLW1qrZ8s5PX2y1W3-zkZy1r9bY5W3y129w3J7-pSW7N_jHd20VSdMVy6krg3Nxy4CW35Kx979ln9YfW6yKF2g2KbJyNW88sGn35_cv_wW4YdC4_1Dq-_QW7lJ2NV9gVWL3W7YG7Jq6pDw4bVzY6CM2T7q26W2wqShp81PvBwV-F5DR1VDSsvN4gFWHdkysmDW6LWbdr4-56m8f5H5X1g04) is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are [open](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV823qgyTW8wLKSR6lZ3lFW7GYhkZ6v6mYrW1ZjW9D80pTxYW1cnW0_2k3ZjWW4VC4K23KqdfZW6WHBKK2p3kfrV9Zz457s847kW48hXJH5CMSqWW710HXG1TZL92W2p7yNQ4m9X8zW5_Vs8q5lD8PxN54Bc6w9x-bPW1DgJ-W5LfpZtW8RtRv97N6DJFW5VRk9n72SmWSW84kcbx8jzjlZW95-jCn93wFKDW2D6v6r5l0_xhW1nm1JX6KYJFnW5-07r55r4JTXW43ZXym3yh-cFN1bHBwJqfkB9W5VZxP53wrq-4W4dPxBW1Y2TmNW2ZVf4V1LjLHBN6D6MtMkhZvjW7240Sq8wB8_BW6__lYQ4FMzTnVVCQX35YSK-4f4090Hj04) except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them [here](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV7s3qgyTW7lCdLW6lZ3pHW79qZkd3yZFbhVdnC3-5MffVYW7bbwz370jGyGW3YF0G03knk9MW7Nd3Rz7MFFtTW3YgYF56dYWT0W4l3h243z4mQ2W4ZxtVk6zrDk0W6pzfCH6SvxFKN1s0JxPNnktpW4pPP2v4rVK1JN84mQ-29xNBmW3cn-Qx20BJC-W1R1vBp8m96nvW6dlzvX7ZqfDHW5bTPZS1J6BrhW6YtX6p8kQW4sW68JW9P66lk-hW84_KHn3DfgvLVYvv6G8cZrQkW2-wz5F6fTHbDW5SRHG85-rjn8W34dVNj9flSZmW3H4Yl27TgnDyf4jGv-s04). ([View Highlight](https://read.readwise.io/read/01jhqdbm9cfyvrxw488kdn28q3))
- DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours — less than 1/10 the [time required to train Llama 3.1 405B](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV7M3qgyTW7Y8-PT6lZ3mwVR7Y0r2S-pPzW5tvByr2MQQd3N5SzwW3R7YFrW5_TZ4B9jTs6XM5z0RL_N_VlW4V7w-w2-KJgKV9wzBh5fLSZwW96f4sf5Yt0C6W75nYwK1GVC2BN6D_Pq2MFWY2N5yzfCNTd7KzN6KWDlF9l4JsW2-vf4T3WxPCjN2j2-vpDcsw7W3nBrgf47BKQXW5t0m8S2q64McW1XFVsz7VgvxkW2vSl8v61pQ-ZW1JDrB-5rlhknW8R92dX8z1gvmW7DKyjk4JH_nqW1YJmC141g2n6V4VQd573vt3zW2k5p0t1pY8mlW4QVqyV8HbPFCW62Gl0748_RL9f3v2_nH04), which DeepSeek-V3 substantially outperforms — at an extraordinarily low cost of $5.6 million. ([View Highlight](https://read.readwise.io/read/01jhqdc093jwsgb992zzhvbd4y))
- In DeepSeek’s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.
  • DeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on [Polyglot](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV7s3qgyTW7lCdLW6lZ3nzW7xk_rl2Dj0qVW8QFBCV1qqctyW4GtvfX523sxkW8s93Cr4xl7JZW4qw2LB6H3D1CW39CXjl3FPCjwW1KRBB61196R8W2hM5Dy6F4KFQW2R-32Y3JSpsVW8xnJ-G39KTmCW2gGTvH1ry6WSN863_7KJSQmPW9jmYsZ5sWN-TMNlVffHjRPVW1s3Zwt95HwNNW275hnw4yzzrwW6G5zff4N9bTxW2dmk0264hrCCW4Hy9mj1_01VTW8QY_nC4JtkZ5W5V181H21lh-ZN469-PBLtdfzN3dh4KR5MHBRVhZxvN87VDWJf6jBqbH04), which tests a model’s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy). 
  • In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others. ([View Highlight](https://read.readwise.io/read/01jhqdcckv5j7efcy0mt496vqk))
- OpenAI’s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows. ([View Highlight](https://read.readwise.io/read/01jhqdcvnyhbydj5j21zb3ba8w))
- Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3’s shockingly low training cost. The team doesn’t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits “meticulous engineering optimizations.”) But it’s likely that DeepSeek’s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn’t employ MoE. In 2022, [Microsoft](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV6z5nR32W5BWr2F6lZ3lhW9lX5nh1QN7MlW2cL7QZ1vDHNDW8b9VPX7BtXRxW7LqCqn5JB6GyF8zNNBYXKL7W95h9QM4DXBc-W2rgnlH2rxbSfN5lNZnq3HT_BW16HFzj35tlJVW8zNKB_85QpgyW6hYlqT1dRL0-W7w5bjG4wLZDmW7TbdLj466q20W3R8srN8cvnFcW5MDhtF8llrm-W8zZyRL5lrnn0VN2BF82h3rX7W2yrPVC6-NsHmMYc6td4hMLpW7m1hR71hLKMGV4Jr4m6xH-7fW3GQ9Rz1wYfd9W5-xY6l5hkHrbW2bDH0-7P7Ww-W3mTKVr4yw-mpN98XqL7v9xppW5tvRpd5WHKycW63NPXR4jRkVyW3RkKGt5VbsdhW3S07xr3rDkPcW15mVtK3mTZzkV-pTMy6qQstbW2V3zcC1_t0GGW15p3tG4LZ0bHf3xZc4-04) found that MoE cost five times less in training for equal performance compared to a dense model, and [Google](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV783qgyTW6N1vHY6lZ3pWW5y_yhC98tDb-W5nj6gl16fvVRW9lpP_B5DhjvMW8r_8Yb8DMDLKV6jmqT12vxsYW67g4mb5psjn_W2-G5Qm7HCSXrW25gW-j6kY94HW4PskcY12ZpSzW3YhRJX47MydpW4hHG207DG6w9N5jHfjqCgmwzN676zdFw6xjwW37xZjb30ZCsJVCld-g47JXGmW64gJS746k6qTW3dj6dK6jfz6RN3Y42cSL1vY_W3D2n-F5Yp2qJW4jdF905SmjhkVT2spn4hV87DW1sX7zW3dRlzvf6CHhv804) and [Meta](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV783qgyTW6N1vHY6lZ3nNW6x-jvv5VtCK-W4kzBkT4w4p4gN6-4qrPB5TMjW37-CNl6cqN_CN1VtWTR2zS-_W1-FvQx4HYY91N30RGFW4z_9sW5WlWW229_LqGW5BW0TF7cPwt2W60hGlq8t1m5jW6b6VG25t-1NTW4t03v62BgL3CN2-2tPPFTJBnW4wbT3z2d5MNLW40_0HW44BH9JW24Nb0f3yqRpVW4gS6Tr3Gft5JW4kT8D47TgdPsW48Myty3DSb_DW7-B8zQ8BMFrwW5wv-C86Lqs5WN2bw58cCCz1wf63v6wq04) reported that MoE achieved better performance than dense models trained on the same numbers of tokens. ([View Highlight](https://read.readwise.io/read/01jhqdd2z6y5n7wrxk6sbh5ryt))
- If they can be replicated, DeepSeek’s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically. ([View Highlight](https://read.readwise.io/read/01jhqddew9e5nydhcgxzvdtd96))
- The United States proposed limits on exports of AI technology that would dramatically expand previous restrictions, creating a new international hierarchy for access to advanced chips and models. ([View Highlight](https://read.readwise.io/read/01jhqddxahh44fx6gm2sw43bs6))
- The Biden administration, which will transition to leadership under incoming President Trump next week, issued new [rules](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV6g5nR32W50kH_H6lZ3p1VcS82n1zTn7SVgfjLr2B0nXGW4Hp8gT66pfqXW3njWpg6nhWgtW53XJdL7DCLV-W3f0sdp2cd6GjN5jTPtLcLH_-W4nVDLT4k4f8yN4Nwlg-BxfczVTXt2l12z4ljW6MD-wh8byMQQW6hSX2d1t9bkVW1d0QkX1KgbPdW2ZQ-Cr5KlplVW4HH2bv1K0b9RW8Fwtyt4yBQFMW1tGqcz2-NsDSN3tR_K-L5lpVW8kZQn01pJtsKW6XQRKS9b9ghWW369sjS7ZNjsjW2nvzgr7NhVtHW1vm5yn1h7b2cW81XjG_6P32-4W2Yw9bB2ms7BPW7drspk44qlB8W5Lh7yB7yKKhSW41YJ4x7w7hX4W3MYSx81r8Q9qW4v63J585HMt5W8zhsWD8LrvPbW7vDh0Y2RHRdqf1mrQGv04) that restrict exports of AI chips and models to most countries beyond a select group of close allies. The rules, which are not yet final, would create a three-tier system that limits exports to a number of close allies and blocks access entirely to China, Iran, North Korea, Russia, and others. They also would introduce the U.S.’ first-ever restrictions on exporting closed weights for large AI models. ([View Highlight](https://read.readwise.io/read/01jhqde38vx0r97nyyv2s8tjk6))
- The restrictions were announced shortly after a [leak](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV823qgyTW8wLKSR6lZ3lmW5F4TkQ3cH-NkW5NPSw86s2NHYW6ZhVgM5hszm7W98vlL91PNHtsW2SJFNm8kYCgzW8CsZQJ9dB1_RW7VKRCv3KPDYFW9b_LPk1GBYwvW8MQCHQ3GxncJW6L_mkZ6bR9RYW8cFFhd1DkdjYM6WLctwFJXdW2GdN7V8lkVdMW1pKfxv3lbRCZW4bhDy25m10bmW4nl5Tw8XpSyDW1qZtHk7_H9bjW1TD3w23gK5wXW8fhHbc2k65xFW97-CW26cygVgW2svDJk1PFNQtW7bPN0X3599N3VrpYkN71MrhNW2jM-8267-3PhW96-Nc02rnmsHW3Q1TGC79Y68_W3CKLsP94R0MDW4YW7831lM8KfdX_QQv04) reached the press. A public comment period of 120 days will enable the incoming U.S. Presidential administration to consider input from the business and diplomatic communities and modify the rules before they take effect. The rules are scheduled to take effect in one year. ([View Highlight](https://read.readwise.io/read/01jhqde9p5y60p7y7xwnp50wbc))
- • A new [hierarchy](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV6g5nR32W50kH_H6lZ3m6W6DhYsL72D0XQW5ThB9m7T75YjN8K2qVjrBNNSW49szTD60F3cbW10LK779gPHNNW6qJNPY6fvTwKW6TxZ8p4W7zZMW4kx7H66_wzrQN5s811Yb-BGHW48wMVb1Spz6WW2dHh7h2BHQ59W7Vk38p43xsD5W86Bg341cGxSWW8S6yX21xLxdzN6d1TxqPPDdFW26wXlw3BM18GW8kZkK82bcdgLN2PJKg46BNrbW4sVq3f8KK-4fW4QsnSM5bjWnZW3K3-gW1BKQXhW2d-S5j1gdTvfW2ys2-r2bW6VNW4mD1k28JSnfqW1PpSMj1W9wLcW19LNk-3ZKV1XW923YJG4sVKYyW8TvmRH1lWTtDW7FNJ8h5Gjw4cV_V-3p5XPgFnW4WHCW95L64knW8j0Cg36nJC1kf1lMZTC04) divides nations into three groups that would have different degrees of access to AI chips both designed in the U.S. and manufactured abroad using U.S. technology, as well as proprietary AI models.
  • **Tier 1:** Australia, Japan, Taiwan, the United Kingdom, and most of Europe would retain nearly unrestricted access. However, these nations must keep 75 percent of their AI computing power within allied countries. No more than 10 percent can be transferred to any single country outside this group to ensure that advanced AI development remains concentrated among close U.S. allies.
  • **Tier 2:** Traditional U.S. allies and trade partners like Israel, Saudi Arabia, and Singapore face an initial cap of 507 million units of total processing power (TPP) — roughly the computational capacity of 32,000 Nvidia H100 chips — through the first quarter of 2025. The cap would increase to 1.02 billion TPP by 2027. U.S. companies that operate in these countries can apply for higher limits: 633 million TPP in Q1 2025, rising to 5.064 billion TPP by Q1 2027.
  • **Tier 3:** China, Russia, and around two dozen other countries are blocked from receiving advanced AI chips, model weights, and specialized knowledge related to these systems. ([View Highlight](https://read.readwise.io/read/01jhqdefys0rap403qpps6btnc))
- The proposed rules build on 2022’s [CHIPS and Science Act](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV7s5nR32W7lCGcx6lZ3kBN4x9-tl_R71KW8dZx8L93XsQPW4p8ZKv4HG-j8W78jvFn3YLn5xW99N2Y37BBqmJN22tCf7KShsHW5tQNxH6bbLkRW7vQPmx1DfJ6-W7mGsYf42_WZpW1bSszb1SZ-LSW6Sd3gK7tS3k7W7WYjR13kXmGVW7XRGB03JgqcrW5dKmlW94WK4QW7PLq653psKgMW54yKCV5t1V9CVm08pC62lhBWVJyk8w3F9yRJW6SH-Sb4vnDvBW1yttZR4v-dYvW62zcTk97tk5WW8DqXrK1p509PW8kvXJ95fPMP4V7yCkR22StPZW66c-T38__QnnW8SW65f2rcmWsW2ll1dP15qRRZW7f4gYl53b3R1W6Tcc0J667MPCW966Qc22x4vLxW90Wj7h6gzjWYW5X8-z06B1_8SN2jpdBrh89Z-W2PBSRt40bbBNW6DXcK_7P6kjtW47ZMqF8cR7xHW4VsDWX3X4Zv9W2QYRZf5QGJ0GN4pvrbSwXcr5W7m70xj3dxS_yf3D_dmK04), which was designed to strengthen domestic semiconductor production and restrict technologies abroad that could bear on U.S. security. An initial round of restrictions in late 2022 [barred](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV823qgyTW8wLKSR6lZ3pCVmpcql5c6WcyW67GTZP59pz1NW13wfCx2Vvm0kW5F4-pL73__83W1TvMGR1l6sYQW9kGnpB1T0K2TW29jZ574WQw7bVtZHCc3sMc9TW86tG3v6ZMXjZW3Mzf738RqNd2W3ctLk02GVW-JW6TMYDM4f0lrlN3wFMR6TnRWsW469LKK5Pg9zqV2l08x4tZwrjV6Nh1m3rpDzDW3fvgNc863nW0W1sy7GF8MR0KYW41M0NF37KXrGVG6T8h97Kc1nW6GH7JH7qMTp5N7SV9V4_4wT7W24v4QQ2BR5wnW2R4TFk6PX3h7V-WY3v4qjV0mW2kGJ-83VPp6ZV3_XyN3bjWVgW3BF1pK7qnpP6f7X5Ytb04) semiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S. [tightened](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV8l3qgyTW95jsWP6lZ3nsW4mC1kZ1Z1LsXW7nzrgX3-RG7NW2tbL_C3SjQgwN68pkZy4BHC-W1zVx8P91TXlSW5wRcj038zPnYW77mzRb6D4C9hW6Z8hgg3d0RL-W7xZhqM3PmS2PW2D2ncx8n0l_YW1_pKnL3p8MVKW8BH9jz9cD4nnW75lHC743kXFPW11_QyH2TgS4bW2sVtRB8F35ynW3DW4y04dkQmhW8g1NfD477nWvW9kY4kD4kJP_QW85BVck3CVQrLW3QGhKg5slLVGW3lkzry5J8bHnVjmRNp1Ss5h3W94GKg48YQ3BsW6X186l7CBTMfW45kYVV3X4hCCW33nQtV6gZHFFW8qknDD4hkrp7N5gxGfB-gtvMW8xylVZ5jWcZ4W378xnY8wP5Rxf4gfqJ204) restrictions further, ordering Taiwan Semiconductor Manufacturing Company, which fabricates those chips, to halt production of advanced chips destined for China. ([View Highlight](https://read.readwise.io/read/01jhqdep0bdhhx2mf5dh2rct2g))
- In addition, President Biden issued an executive order to encourage the rapid build-out of computing infrastructure for AI. The federal government will hold competitions among private companies to lease sites it owns for the building of data centers at private expense. The selection of sites will take into account availability of sources of clean energy, including support for nuclear energy. The government will expedite permitting on these sites and support development of energy transmission lines around them. It will also encourage international allies to invest in AI infrastructure powered by clean energy. ([View Highlight](https://read.readwise.io/read/01jhqdesphxh90fq76z5rj2ebn))
- Protecting the United States’ advantages in high tech has been a rising priority for the White House over the past decade. The earlier export restrictions forced many Chinese AI developers to rely on less-powerful hardware. The new limits are likely to have a far broader impact. They could force developers in the Tier 2 and Tier 3 countries to build less resource-intensive models and lead them to collaborate more closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which have [warned](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV7s3qgyTW7lCdLW6lZ3mnW16H_3B1Q58KLW3L5-cD6KJWjJW5-SjmP4MDXwsW8B57zX7pR2BzW1Yd3kq3Dk2qkW701S5K49nNBGW3f_2zq61DxH3W8Ww2Sx1fsjKjVc5rBn4YbxR-W7cxTMh8Vvr9-W40C9SK3J90Z2W4sGNvL8GW8sZW8PDFRX5n-FRVW2D2QSt25NynTW24XYp45FSVhxW43ZVb92d6f3LW89nS592T3mLFW661ZRQ77NMYHW1y1KXh7sX886W7Lz24h8kQ8trW5l4n-X7gv8TRW8WrFTN1xHt8pW23V0Zy8TXDHGN1xpVD8T8n7hf3Dr37204) that the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations to [reconsider](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV823qgyTW8wLKSR6lZ3nyW7tXPX27b0z_wW2HnRy59kSMtWW3Gw7B01y2LkdW8nqmNF6PJNHzW28vv1M3lrzkmW6vK7XL6j6VGDW9dckQd7SN9tZW93DQRq4g99G4VQ8Vc83MS3dkW73_hvj5xgYXLW1bhftT5ZzK9QN4C0JbKqJjT1W4BvBFS6vjrDrW1hL1td2nX04zW90Bpvk6cklyWW7ss24r2YpgSQN1CNLwT_bdKjW14_PJx73BXtdN4vG6jJhK4jtW2tCdRm3vxKXhN39NBq-Jn8rjW6vcW4y8d4h1MW3S_Pns2c3XwSW115zFy6NWjK9V-CjTh8S-jMKW7g-Vzd8jb8LxN88ns1xJbwFqN7yxGjHGNCVCf908h9z04) their plans. ([View Highlight](https://read.readwise.io/read/01jhqdexwjx2ttrhc57kr1pg9r))
- The Biden administration’s embargo on AI chips has been [leaky](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV8l3qgyTW95jsWP6lZ3mdW2XDPWf7C0XpZVQQSQC8c1P4bVbNGTp8v0JnDW7svd4d7qwL4SW48RdJX5YK95CW35kXfZ4PfmPHW8WwPQs4hxkWzN2NBZp2bgxF3W4dpF-f3L8F1NW1mtYlW39m_35W6xrfSN3HHqfXW1HBv9w4sCFPvW5_y9C21gsWS1W9d7wFQ5Zc29TW50F9My8P0VtBW3Kcn-C2XQrWfW2kv79N5DqtplW2dJZ3M6YSFGWVGGH7742wSC4W8KB7fs8Y3TvCN19S9rbxs-hTW5Dr1fg4bDrx_W8jpXrm34Kf_JW3nxNHP42kQdHMth65L2kW9gW1qQvqR5q5w99W2tRQ_05M5RN_W5Rh4Lg3gJcK7W7LpGGS4qVDGRN6-jhzGCRYkrf6tJcn-04). So far, it has slowed down adversaries only slightly while spurring significant investment in potential [suppliers](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VW6M5F7lXnsgW6ntrs29hkFQ5W8ZpTlp5qTZZFN3wqV8l3qgyTW95jsWP6lZ3l2W2dw8R44WMLy0W7rTgf045h2lhW5Js2kb4KG17ZW8_jM1w1TS6n1W3Kv9y279K-HCW6sl8L96PPDrzW8VXPW32G69m1W7ShC_Y3M4MYxN78QB7GzXtkMW4FQRNS7Psxq_W1PQ4KR6kS9GQW6Whv7Z48mNwWW4-h3vB2CnnT-W4L6ZSC1XJF1QW5zCMp62Fh8m8W4yK34X1-W45CW1r09hW10MDKDW7pbtPs2NjplYW99CZyc2PlgrrW5H_qnT2t3T4kVlvrF-5LtTp0W96GRDY5gJK57W87G3Xr5ZFmLxN1Wc6zLMX_QYW5dq10G4_-sC6W6ZB0j06ZVxtBW97gLrb7DLFTWVXPGkV8dtH26W2X3_zL6cnmryN2QrC-Nj_s5zf30JN6C04) that aren’t connected to the U.S. While the public comment period invites lobbying and industry feedback, ultimately geopolitical priorities may hold sway. Whatever the outcome, reducing the world’s dependence on U.S. chips and models would result in a very different global AI ecosystem. ([View Highlight](https://read.readwise.io/read/01jhqdf0dfvzfg324rws20thnn))

