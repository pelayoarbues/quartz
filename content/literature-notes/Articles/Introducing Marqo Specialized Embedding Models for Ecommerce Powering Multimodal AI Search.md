---
author: [[marqo.ai]]
title: "Introducing Marqo Specialized Embedding Models for Ecommerce: Powering Multimodal AI Search"
date: 2024-11-14
tags: 
- articles
- literature-note
---
![rw-book-cover](https://cdn.prod.website-files.com/630ca238d7b194730630c5ee/672e3cdbd17f2bce96eef737_ecommerce-launch.png)

## Metadata
- Author: [[marqo.ai]]
- Full Title: Introducing Marqo Specialized Embedding Models for Ecommerce: Powering Multimodal AI Search
- Document Note: Q: What could be the impact of having specialised models like this one for marketplaces? Answer in less than 300 chars 
   A: Specialized models like Marqo-Ecommerce can significantly enhance search accuracy and speed in marketplaces, leading to improved customer satisfaction and higher conversion rates. By delivering more relevant product recommendations and search results, they can boost sales and customer retention.
- URL: https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models

## Highlights
- We've developed two new state-of-the-art foundation models for generating multimodal product embeddings from images and text, [**Marqo-Ecommerce-B**](https://huggingface.co/Marqo/marqo-ecommerce-embeddings-B) and [**Marqo-Ecommerce-L**](https://huggingface.co/Marqo/marqo-ecommerce-embeddings-L) . These models outperform existing state-of-the-art solutions like [Amazon Titan's Multimodal Embedding](https://aws.amazon.com/about-aws/whats-new/2023/09/amazon-titan-embeddings-generally-available/) by up to **88%** and the best open source model ([ViT-SO400M-14-SigLIP](https://huggingface.co/timm/ViT-SO400M-14-SigLIP)) by up to **31%**. ([View Highlight](https://read.readwise.io/read/01jcpbm59rrreps9jw16apgax7))
- We benchmarked a number of embedding models for multimodal product retrieval. These included the base models [**`ViT-B-16-SigLIP`**](https://huggingface.co/timm/ViT-B-16-SigLIP) and [**`ViT-L-16-SigLIP`**](https://huggingface.co/timm/ViT-L-16-SigLIP-384), as well as the [best open source](https://www.marqo.ai/blog/benchmarking-models-for-multimodal-search) CLIP/SigLIP model, [**`ViT-SO400M-14-SigLIP`**](https://huggingface.co/timm/ViT-SO400M-14-SigLIP-384). We also included API-based multimodal embeddings offered by [Amazon-Titan-Multimodal](https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-titan-multimodal-embeddings-model-bedrock/), [GCP-Vertex](https://cloud.google.com/vertex-ai), [Jina-V1-CLIP](https://huggingface.co/jinaai/jina-clip-v1), and [Cohere-Embedding-v3](https://cohere.com/blog/introducing-embed-v3), although there are limitations on some of these private providers (see below for more details). ([View Highlight](https://read.readwise.io/read/01jcpbmdgxh15e8wqp7p7va6kt))
- Our benchmarking process was divided into two distinct regimes, each using different datasets of ecommerce product listings: **`marqo-ecommerce-hard`** and **`marqo-ecommerce-easy`**. Both datasets contained product images and text and only differed in size. The "easy" dataset is approximately 10-30 times smaller (200k vs 4M products), and designed to accommodate rate-limited models, specifically Cohere-Embeddings-v3 and GCP-Vertex (with limits of 0.66 rps and 2 rps respectively). The "hard" dataset represents the true challenge, since it contains four million ecommerce product listings and is more representative of real-world ecommerce search scenarios. For both the **`marqo-ecommerce-hard`** and **`marqo-ecommerce-easy`** datasets, the models were benchmarked on three different tasks: ([View Highlight](https://read.readwise.io/read/01jcpbmkj66fnxhn8qspwkbawe))
- • **GoogleShopping**-**Text2Image**: uses the product title to search product images from [Google Shopping data](https://arxiv.org/abs/2404.08535). This is representative of descriptive queries in search. 
  ‍
  • **GoogleShopping**-**Category2Image**: uses the product categories as queries to search product images from Google Shopping data. This is analogous to short keyword like queries in search. 
  ‍
  • **AmazonProducts**-**Text2Image**: uses the product title to search product images from [Amazon product data](https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews). This is representative of descriptive queries in search.
  We have made these datasets available on [Hugging Face](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb) along with [scripts](https://github.com/marqo-ai/marqo-ecommerce-embeddings/tree/main/scripts) to reproduce the evaluation.
  The benchmarking results show that the **Marqo-Ecommerce models** consistently outperformed *all other models* across various metrics. Specifically, **`marqo-ecommerce-L`** achieved an average improvement of **17.6% in MRR** and **20.5% in nDCG@10** when compared with the current best open source model, **`ViT-SO400M-14-SigLIP`** across all three tasks in the **`marqo-ecommerce-hard`** dataset. When compared with the best private model, **`Amazon-Titan Multimodal`**, we saw an average improvement of **38.9% in MRR** and **45.1% in nDCG@10** across all three tasks, and **35.9% in Recall** across the Text-to-Image tasks in the **`marqo-ecommerce-hard`** dataset. ([View Highlight](https://read.readwise.io/read/01jcpbncsfvfz5d1cxtszr5wc0))
- While contrastive learning models like [CLIP](https://arxiv.org/abs/2103.00020) and [SigLIP](https://arxiv.org/abs/2303.15343) are powerful, they are not optimized for the needs of ecommerce. They were trained on a large collection of images, many of which aren't related to ecommerce, with little curation or domain specificity. The product data in ecommerce datasets differs significantly from general-purpose datasets, resulting in suboptimal performance when these models are used for search and recommendations. Additionally, these models were trained on data that is now several years old, and they have no understanding of recent products or trends. ([View Highlight](https://read.readwise.io/read/01jcpbnpn6yb4fa7hjmkkccyhj))
- We built [**Marqo-Ecommerce-B**](https://huggingface.co/Marqo/marqo-ecommerce-embeddings-B) and [**Marqo-Ecommerce-L**](https://huggingface.co/Marqo/marqo-ecommerce-embeddings-L) models, which excel at ecommerce search, retrieval, and recommendation tasks. The models were trained on 100s of millions of samples from ~50 million unique products across 20,000 Amazon asin categories spanning from appliances to automotive to office products to pet supplies. The models were evaluated on extensive benchmark datasets that spanned over 4 million unique products covering the 20,000 categories. The categories are taken from Amazon’s product taxonomy. ([View Highlight](https://read.readwise.io/read/01jcpbnvb69b9v79pgdjk66zqr))
- The Marqo-Ecommerce embedding models are designed specifically to work seamlessly with [Marqo Cloud](https://www.marqo.ai/cloud), our end-to-end embeddings platform. Additionally, you can fine tune our embedding models on your own product catalogs and user behavior, using [Marqtune](https://www.marqo.ai/blog/getting-started-with-marqtune). Marqtune is our embedding model training platform backed by our contrastive learning framework - [GCL](https://arxiv.org/abs/2404.08535). ([View Highlight](https://read.readwise.io/read/01jcpbp3fpet51g7dj2kx7gch3))
- We've released both our models, [**Marqo-Ecommerce-B**](https://huggingface.co/Marqo/marqo-ecommerce-embeddings-B) and [**Marqo-Ecommerce-L**](https://huggingface.co/Marqo/marqo-ecommerce-embeddings-L) on Hugging Face. The B model is smaller and faster for inference (with times of 5.1 ms for single batch text, and 5.7 ms for image) and a smaller embedding dimension (768). The L model is larger (652M parameters), has a larger embedding dimension (1024), but has better retrieval performance. Marqo-Ecommerce-L has up to **7.3% MRR** and **7.4% nDCG@10** average improvement over Marqo-Ecommerce-B across the three tasks on the 4M evaluation dataset. ([View Highlight](https://read.readwise.io/read/01jcpbpj27tjejss9b389p6vdv))
- Here are the detailed results for three general ecommerce retrieval tasks. These tasks measure the performance of various embedding models in retrieving images based on long and short text descriptions and categories. We focus on [Precision](https://www.marqo.ai/blog/what-is-precision-in-machine-learning), [Recall](https://www.marqo.ai/blog/what-is-recall-in-machine-learning), [MRR (Mean Reciprocal Rank)](https://www.marqo.ai/blog/what-is-mrr-in-machine-learning), and [nDCG](https://www.marqo.ai/blog/what-is-normalized-discounted-cumulative-gain-ndcg) to showcase how our Marqo-Ecommerce models stack up against existing solutions, such as Amazon Titan Multimodal and other popular open-weights SigLIP ViT models from Google. ([View Highlight](https://read.readwise.io/read/01jcpbq7pfbvy6nvy9cdmr6fd6))
- With the release of Marqo-Ecommerce-B and Marqo-Ecommerce-L, ecommerce platforms now have access to powerful, purpose-built embedding models that outperform existing solutions by up to 88%. These models are specifically tailored for the unique challenges of ecommerce, delivering highly accurate retrieval results, whether it's matching product titles to images or associating products with broader categories. The Marqo-Ecommerce models are set to transform search, retrieval, and recommendation tasks in the ecommerce industry. ([View Highlight](https://read.readwise.io/read/01jcpbqvbv7kkxff9c3730b8hy))
