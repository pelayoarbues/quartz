---
author: [[Elie Bakouch]]
title: "SmolVLM - Small Yet Mighty Vision Language Model"
date: 2024-11-26
tags: 
- articles
- literature-note
---
![rw-book-cover](https://huggingface.co/blog/assets/smolvlm/banner.png)

## Metadata
- Author: [[Elie Bakouch]]
- Full Title: SmolVLM - Small Yet Mighty Vision Language Model
- URL: https://huggingface.co/blog/smolvlm

## Highlights
- This blog post introduces SmolVLM, a 2B VLM, SOTA for its memory footprint. SmolVLM is small, fast, memory-efficient, and fully open-source. All model checkpoints, VLM datasets, training recipes and tools are released under the Apache 2.0 license. ([View Highlight](https://read.readwise.io/read/01jdn10jggtsyv5ev7hnnedpf1))
- This year has seen a boom in multimodal AI with many large vision language models released. The trends were to initially scale up compute, later scale up the data diversity by generating synthetic data with large models, and, recently, scale down to make these models more efficient. Small open models allow local deployment to browser or edge devices, cut inference costs, and enable user customization. Some notable examples of these models include PaliGemma 3B, moondream2, and Qwen2VL. ([View Highlight](https://read.readwise.io/read/01jdn10nwbmzn27kzr3nvmrrjn))
- In this blog post, we introduce [SmolVLM](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct), a new family of 2B small vision language models that can be used commercially and deployed to smaller local setups, with completely open training pipelines. ([View Highlight](https://read.readwise.io/read/01jdn10stp7kryb773gre9kzf4))
- We release three models: [SmolVLM-Base](https://huggingface.co/HuggingFaceTB/SmolVLM-Base), which can be used for downstream fine-tuning, [SmolVLM-Synthetic](https://huggingface.co/HuggingFaceTB/SmolVLM-Synthetic), the fine-tuned variant on synthetic data, and [SmolVLM Instruct](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct), the fine-tuned instruction variant, which can be used out of the box for interactive end-user applications. ([View Highlight](https://read.readwise.io/read/01jdn10w2hs1p8dp6bxj933kbj))
- This release comes with open-source models integrated into transformers, [a demo built on SmolVLM Instruct](https://huggingface.co/spaces/HuggingFaceTB/SmolVLM), and a supervised fine-tuning script. We have used the datasets previously used for Idefics3: [the Cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron) and [Docmatix](https://huggingface.co/datasets/HuggingFaceM4/Docmatix), which are also fully open-source. ([View Highlight](https://read.readwise.io/read/01jdn115f6t1h9961ta8w0qtkb))
- For SmolVLM, we closely followed the architecture from Idefics3, to the point that we use the same implementation in transformers. There are, however a few key differences: ([View Highlight](https://read.readwise.io/read/01jdn11fc0p87erfe6h8p9f5dr))
- ‚Ä¢ We replaced Llama 3.1 8B with SmolLM2 1.7B as the language backbone.
  ‚Ä¢ We more aggressively compress the patched visual information by reducing the information 9x using the pixel shuffle strategy, compared to 4x with idefics3.
  ‚Ä¢ We use patches of 384*384, instead of 364x364, because 384 is divisible by 3, which is necessary for our pixel shuffle strategy to work.
  ‚Ä¢ For this, we change the vision backbone to use shape-optimized SigLIP with patches of 384x384 pixels and inner patches of 14x14. ([View Highlight](https://read.readwise.io/read/01jdn11qtevy9phckgtz3zqkn9))
- SmolVLM provides the best memory usage among the existing suite of vision language models in transformers. This allows it to run efficiently on-device, such as a laptop! You can see above the GPU memory usage in GBs for each model, running inference with one or two input images, and using the same images and text prompts in all tests. SmolVLM's efficiency in image encoding is built into the model. SmolVLM encodes each 384x384 image patch to 81 tokens. This results in SmolVLM encoding our test prompt and a single image in 1.2k tokens, whereas Qwen2-VL uses 16k tokens. This also explains why the memory consumption increases so much for 2 images with Qwen and InternVL. In contrast, the increase is much more moderate for SmolVLM and PaliGemma, which use a similar approach. ([View Highlight](https://read.readwise.io/read/01jdn11ve70smmvaebqvym4q5e))
- SmolVLM's tiny memory footprint also implies that it requires far fewer computations to prefill the model and generate. Compared to Qwen2-VL, the prefill throughput is 3.3 to 4.5 times faster, and the generation throughput is 7.5 to 16 times faster. ([View Highlight](https://read.readwise.io/read/01jdn122rwa21gz7dgtsdpzrsh))
- Given SmolVLM's long context and the possibility of tweaking the internal frame resizing of the model, we explored its suitability as an accessible option for basic video analysis tasks, particularly when computational resources are limited. ([View Highlight](https://read.readwise.io/read/01jdn127scc6bskwn7cgr3fwaa))
- In our evaluation of SmolVLM's video understanding capabilities, we implemented a straightforward [video processing pipeline code](https://github.com/huggingface/smollm/blob/main/inference/smolvlm/SmolVLM_video_inference.py), extracting up to 50 evenly sampled frames from each video while avoiding internal frame resizing. This simple approach yielded surprisingly competitive results on the CinePile benchmark, with a score of 27.14%, a performance that positions the model between InterVL2 (2B) and Video LlaVa (7B). ([View Highlight](https://read.readwise.io/read/01jdn129a752427230gnfm7rzh))
- While in the second question, we see some temporal understanding limitations (the cook points to one ingredient after the other rather than pointing/holding all of them at the same time) SmolVLM demonstrated great scene understanding and object recognition capabilities. ([View Highlight](https://read.readwise.io/read/01jdn12gdkq68rgb2ha1dfq1j7))
- You can easily load SmolVLM using the `Auto` classes in transformers. Under the hood, the model and processor are mapped to the same implementations used for Idefics3. ([View Highlight](https://read.readwise.io/read/01jdn12negwfet14gw2zd7nzrb))
- Image and text can be interleaved arbitrarily, and you can pass in multiple images. Here‚Äôs how you can use the chat template and pass in the formatted input to the processor. ([View Highlight](https://read.readwise.io/read/01jdn12rjxdv954w0bq8s847qs))
- SmolLM2‚Äôs pre-training context window is insufficient for VLMs. Images are encoded into many tokens, and we wanted to support multiple images. To address this, we extended it to 16k tokens by increasing the RoPE base value from 10k to 273k, following the guidelines in [‚ÄúScaling Laws of RoPE-based Extrapolation‚Äù](https://arxiv.org/abs/2310.05209). We fine-tuned the model on a mixture of long- and short-context datasets. For long-context datasets, we used the ‚Äúbooks‚Äù subset of Dolma (primarily Project Gutenberg) and code documents with 8k+ tokens from The Stack, each contributing 20% to the final mixture. For short-context datasets, we streamlined the original SmolLM2 pre-training mix to include 20% FineWeb-Edu, 20% DCLM, and 20% from our math dataset (to be released soon). The math dataset was upsampled to mitigate a performance drop observed on GSM8k during the context extension process. All experiments were implemented using the [EasyContext repository](https://github.com/jzhang38/EasyContext). ([View Highlight](https://read.readwise.io/read/01jdn12wty7m80rx1z90dg650x))
- To select the optimal checkpoint, we created a single metric by combining these benchmarks with different manually assigned weights to reflect their relative importance in assessing the model's capabilities. We used this single metric to select the best checkpoint. Generally, the models tended to do great on most benchmarks with more training, but their relative performance on DocVQA would decrease considerably. ([View Highlight](https://read.readwise.io/read/01jdn13k79m4twhrvsjkp7en45))
- You can fine-tune SmolVLM using transformers and apply alignment techniques using TRL üöÄ ([View Highlight](https://read.readwise.io/read/01jdn13n7dxwqcaz13gth78ek2))
- We provide a [notebook](https://github.com/huggingface/smollm/blob/main/finetuning/Smol_VLM_FT.ipynb) to fine-tune it on the VQAv2 dataset, optionally using LoRA, QLoRA or full fine-tuning. In the notebook, you can find some tricks to save up even more memory and have a larger batch size to fit SmolVLM inside consumer GPUs, like L4, for training. With batch sizes of 4, 8-bit loading with QLoRA and gradient checkpointing we can fine-tune in L4, and it consumes around ~16 GBs of VRAM. This makes it possible to fine-tune your SmolVLM using Colab! You can play around with the parameters to get a nice point in training duration-memory trade-off. ([View Highlight](https://read.readwise.io/read/01jdn13prqp0j2xgchdy5xn2zz))
- We introduced SmolVLM, a fully open, small, and mighty VLM for the community! We also provide tools for the community to use and customize it. We are looking forward to seeing what you will create with SmolVLM. ([View Highlight](https://read.readwise.io/read/01jdn13sb7dwm6hyrb34g8yfnd))
